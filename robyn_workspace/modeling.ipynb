{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "from datetime import datetime, date, time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from PIL import Image\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this file, instructions how to approach the challenge can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on different types of Machine Learning problems:\n",
    "\n",
    "- **Regression Problem**: The goal is to predict delay of flights.\n",
    "- **(Stretch) Multiclass Classification**: If the plane was delayed, we will predict what type of delay it is (will be).\n",
    "- **(Stretch) Binary Classification**: The goal is to predict if the flight will be cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository of functions I've generated to use on the larger datasets in preparation of machine learning\n",
    "def whatsleft(df): # useful for checking data clean-up status\n",
    "    '''Functions like df.isnull().sum() except only shows the remaining columns with values missing'''\n",
    "    nan_cols = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "    print('Column \\t\\t # Nan Values')\n",
    "    return df[nan_cols].isnull().sum()\n",
    "# ex: whatsleft(flight10k)\n",
    "\n",
    "def rem_outliers(df, col):\n",
    "    ''' Remove outliers which fall outside of 3 standard deviations above and below the mean of the data set\n",
    "            Input\n",
    "                (0) dataframe containing the data\n",
    "                (1) column to remove outliers from\n",
    "            Output\n",
    "                rows of df which are classified as outliers in the specified column are directly removed\n",
    "                print out stating count of outliers removed '''\n",
    "    mean, cutoff = np.mean(df[col]), np.std(df[col]) * 3   # 3 stddev outside the mean\n",
    "    lower, upper = mean - cutoff, mean + cutoff\n",
    "    outliers = [x for x in df[col] if x < lower or x > upper]\n",
    "    df.drop(df[(df[col] > upper) | (df[col] < lower)].index, inplace=True)\n",
    "    return f'{len(outliers)} outliers removed'\n",
    "# ex: rem_outliers(flight10k, 'arr_delay')\n",
    "\n",
    "def get_state(df, col, new_col):\n",
    "    '''split the string of City, State text to grab only the end state code (2 letters) regardless of city name length\n",
    "                Input:\n",
    "                    (0) dataframe containing information\n",
    "                    (1) col = column containing City, State Code information to pull state from\n",
    "                    (2) name of the new column which will be appended to the end of the df\n",
    "                Output:\n",
    "                    df[new_col] containing state code only'''\n",
    "    value=[]\n",
    "    for i in df[col]:\n",
    "        i = str(i)\n",
    "        value.append(i[-2:])\n",
    "    df[new_col] = value        \n",
    "# ex: get_state(flight10k, 'origin_city_name', 'state_origin')\n",
    "\n",
    "def fast(df):\n",
    "    '''If the arrival delay time (minutes) is less than the departure delay, \n",
    "            then return 1 for 'fast' since the plane must have gone faster to make-up for the delayed departure\n",
    "            else return 0 for 'not fast' as the arrival delay was as long or longer than departure delays'''\n",
    "    if df['dep_delay'] > df['arr_delay']:\n",
    "        return 1 # sped up during flight\n",
    "    else: \n",
    "        return 0 # remained behind schedule\n",
    "# ex: delays['fast'] = delays.apply(fast, 1)\n",
    "\n",
    "def haul(df, col):\n",
    "    '''Determine if flight length is SHORT, MEDIUM or LONG based on expected elapsed flight time. \n",
    "            Input \n",
    "            (0) df containing flight information, \n",
    "            (1) column containing the elapsed flight time in minutes\n",
    "                  \n",
    "            Output\n",
    "            'haul_length' column determining haul length category per row in df'''\n",
    "    length=[]\n",
    "    for i in df[col]:\n",
    "        if i < (3*60): # up to 3 hours\n",
    "            length.append(0) # 0 = SHORT HAUL\n",
    "        elif (i >= (3*60)) and (i < (6*60)): # 3-6 hours\n",
    "            length.append(1) # 1 = MEDIUM HAUL\n",
    "        elif i >= (6*60):# 6+ hours\n",
    "            length.append(2) # 2 = LONG HAUL\n",
    "    df['haul_length'] = length\n",
    "# ex: haul(flight10k, 'crs_elapsed_time')\n",
    "    \n",
    "def gethour(df,col):\n",
    "    '''Convert hhmm to hh (24-hr) hour-only output\n",
    "            Input \n",
    "            (0) df containing flight information, \n",
    "            (1) column containing the hhmm time\n",
    "                  \n",
    "            Output\n",
    "            rewrite on input column in rounded hh format'''\n",
    "    values = []\n",
    "    for i in df[col]:\n",
    "        mins = (i % 100) / 60 \n",
    "        hour = i // 100\n",
    "        hh = round(hour+mins)\n",
    "        values.append(hh)\n",
    "    df[col] = values\n",
    "# ex: gethour(flight10k, 'crs_dep_time')\n",
    "\n",
    "def time_day(df, col):\n",
    "    ''' Input:\n",
    "            (0) df containing flight information\n",
    "            (1) corresponding column of time of flight (i.e. departure or arrival) (format hhmm)\n",
    "        Output:\n",
    "            re-write of time column into categorical MORNING, AFTERNOON, EVENING, or OVERNIGHT'''\n",
    "    gethour(df, col)\n",
    "    timeday = []\n",
    "    for i in df[col]:\n",
    "        if (i>=23) or (i<5):\n",
    "            timeday.append(0) # 0 = OVERNIGHT\n",
    "        elif (i>=5) and (i<12):\n",
    "            timeday.append(1) # 1 = MORNING\n",
    "        elif (i>=12) and (i<18):\n",
    "            timeday.append(2) # 2 = AFTERNOON\n",
    "        elif (i>=18) and (i<23):\n",
    "            timeday.append(3) # 3 = EVENING\n",
    "    return timeday\n",
    "# ex: time_day(flight10k, 'crs_dep_time')\n",
    "\n",
    "# UPDATED C - need same as below, a pre-defined listing of the airports on these lists so we can model unknown with it\n",
    "def busiest_airports(flight_df, airport_col):\n",
    "    ''' Input:\n",
    "            (0) dataframe of flight information\n",
    "            (1) corresponding column containing airport codes (i.e. origin or destination)\n",
    "        Output \n",
    "            new column of airport ranking of business <-- get this into csv file\n",
    "                0 - not busy airports\n",
    "                1 - slightly busy\n",
    "                2 - moderately busy\n",
    "                3 - busy\n",
    "                4 - top ~10 heavy air traffic airports '''\n",
    "    # step one, new df containing airport code and count of flights -- grabbing only top10\n",
    "    df = pd.DataFrame(flight_df[airport_col].value_counts()).reset_index().rename(columns={'index':'airport',airport_col:'count'})\n",
    "    # step two, calculate percentage of total air traffic for each airport -- don't want to make cut offs as numbers but percentage of air traffic\n",
    "    perc_list = []\n",
    "    for x in df['count']:\n",
    "        perc_list.append((x / flight_df[airport_col].count()) * 100)\n",
    "    df['p_airtraffic'] = perc_list\n",
    "    df.drop(columns='count', inplace=True)\n",
    "    busy_dict = {}\n",
    "    for i,j in zip(df['airport'],df['p_airtraffic']):\n",
    "        if j < 0.012:\n",
    "            busy_dict[i] = 0 # bottom 25% of airports 'not busy'\n",
    "        elif (j>=0.012) and (j<0.04):\n",
    "            busy_dict[i] = 1 # 50% range of airports 'slight busy'\n",
    "        elif (j>=0.04) and (j<0.152):\n",
    "            busy_dict[i] = 2 # 75% range of airports 'moderately busy'\n",
    "        elif (j>=0.152) and (j<2.2):\n",
    "            busy_dict[i] = 3 # up to top ~10 airports 'busy'\n",
    "        elif (j>=2.2):\n",
    "            busy_dict[i] = 4 #  top ~10 of airports 'heavy air traffic'\n",
    "    #flight_df[f'busy_{airport_col}'] = np.nan\n",
    "    #flight_df[f'busy_{airport_col}'] = flight_df[f'busy_{airport_col}'].fillna(flight_df[airport_col].map(busy_dict))\n",
    "# ex: busiest_airports(flights,'origin')\n",
    "    return busy_dict\n",
    "def busy_rating(df, airport_col):\n",
    "    traffic_rating = pd.DataFrame.from_dict(busiest_airports(df, airport_col), orient='index', columns=[f'busy_{airport_col}']).reset_index().rename(columns={'index':f'{airport_col}'})\n",
    "    traffic_rating.to_csv(f'data/{airport_col}_traffic_rating.csv')\n",
    "# ex: busy_rating(flights, 'origin')\n",
    "\n",
    "# UPDATED A - this outputs the dictionary which was saved as csv for all future data use\n",
    "def airline_delay(df, dep_delay, airline_col):\n",
    "    ''' function which looks at the average dep_delay for each airline and gives ranking of expected delay based on airline'''\n",
    "    stats = pd.DataFrame(df[dep_delay].groupby(df[airline_col]).mean()).sort_values(dep_delay, ascending=False).reset_index()\n",
    "    rating = {}\n",
    "    for i,j in zip(stats[airline_col], stats[dep_delay]):\n",
    "        if (j < 3):\n",
    "            rating[i] = 0 # no delays on average (~25% q1 of means)\n",
    "        elif (j >= 3) and (j < 6):\n",
    "            rating[i] = 1 # low delays on average (~50% q2 of means)\n",
    "        elif (j >= 6) and (j < 9):\n",
    "            rating[i] = 2 # medium delays on average (~75% q3 of means)\n",
    "        elif (j >= 9):\n",
    "            rating[i] = 3 # high delays on average (>75% q3 of means)\n",
    "    #df['airline_delay'] = np.nan\n",
    "    #df['airline_delay'] = df['airline_delay'].fillna(df[airline_col].map(rating))            \n",
    "# ex: airline_delay(flight10k, 'dep_delay', 'op_unique_carrier')\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to generate the data for 2018-2019 information can be found in Database_Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task: Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **ARR_DELAY**. We need to be careful which columns to use and which don't. For example, DEP_DELAY is going to be the perfect predictor, but we can't use it because in real-life scenario, we want to predict the delay before the flight takes of --> We can use average delay from earlier days but not the one from the actual flight we predict.  \n",
    "\n",
    "For example, variables **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY** shouldn't be used directly as predictors as well. However, we can create various transformations from earlier values.\n",
    "\n",
    "We will be evaluating your models by predicting the ARR_DELAY for all flights **1 week in advance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGIC: after we generate all the different model types, how will we produce an end result? ENSEMBLING [Source](https://towardsdatascience.com/two-is-better-than-one-ensembling-models-611ee4fa9bd8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble=VotingClassifier(estimators=[('Decision Tree', decisiontree), ('Random Forest', forest)], \n",
    "                       voting='soft', weights=[2,1]).fit(train_X,train_Y)\n",
    "print('The accuracy for DecisionTree and Random Forest is:',ensemble.score(test_X,test_Y))\n",
    "# We can assign weights depending on the performance or take an average, ie setting equal weights for the algorithms.\n",
    "# Bag, Boost or Stack algorithms to combine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning of the 250K Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers = pd.read_csv('data/passengers250K.csv', index_col=0)\n",
    "flights = pd.read_csv('data/flights250K.csv', index_col=0)\n",
    "\n",
    "# remove empty flanking column\n",
    "flights.drop(columns='no_name', inplace=True)\n",
    "# if flight cancelled, remove from dataset since we want to prdecit flights that ARE leaving\n",
    "flights = flights[flights['cancelled'] == 0] # 0 meaning not cancelled\n",
    "flights.drop(columns=['cancelled','cancellation_code'], inplace=True)\n",
    "# time count columns NaN to be filled with 0 since no time elapsed\n",
    "time_counts = ['dep_delay','arr_delay','carrier_delay','weather_delay','nas_delay','security_delay','late_aircraft_delay','total_add_gtime','longest_add_gtime','taxi_out','taxi_in']\n",
    "flights[time_counts] = flights[time_counts].fillna(0)\n",
    "# filling remaining empty NaN time columns with related columns information (i.e. wheels_off == dep_time/wheels_on == arr_time/actual_elapsed_time == CRS_elapsed_time) plotted earlier\n",
    "flights['wheels_off'] = flights['wheels_off'].fillna(flights['crs_dep_time'])\n",
    "flights['wheels_on'] = flights['wheels_on'].fillna(flights['crs_arr_time'])\n",
    "flights['arr_time'] = flights['arr_time'].fillna(flights['crs_arr_time'])\n",
    "flights['actual_elapsed_time'] = flights['actual_elapsed_time'].fillna(flights['crs_elapsed_time'])\n",
    "flights['air_time'] = flights['air_time'].fillna(flights['crs_elapsed_time'])\n",
    "# 9778 / 9855 empty =  99+% NaN -- let's drop this first_dep_time column\n",
    "flights.drop(columns='first_dep_time', inplace=True)\n",
    "# THIS CAN BE CHANGED\n",
    "#columns never used to drop\n",
    "flights.drop(columns=['mkt_unique_carrier','tail_num','branded_code_share','mkt_carrier','mkt_carrier_fl_num','op_carrier_fl_num','origin_airport_id','dest_airport_id','diverted','dup','flights','origin_city_name','dest_city_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering will play a crucial role in this problems. We have only very little attributes so we need to create some features that will have some predictive power.\n",
    "\n",
    "- weather: we can use some weather API to look for the weather in time of the scheduled departure and scheduled arrival.\n",
    "- statistics (avg, mean, median, std, min, max...): we can take a look at previous delays and compute descriptive statistics\n",
    "- airports encoding: we need to think about what to do with the airports and other categorical variables\n",
    "- time of the day: the delay probably depends on the airport traffic which varies during the day.\n",
    "- airport traffic\n",
    "- unsupervised learning as feature engineering?\n",
    "- **what are the additional options?**: Think about what we could do more to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>airline</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>taxi_out</th>\n",
       "      <th>wheels_off</th>\n",
       "      <th>wheels_on</th>\n",
       "      <th>taxi_in</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>actual_elapsed_time</th>\n",
       "      <th>air_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>carrier_delay</th>\n",
       "      <th>weather_delay</th>\n",
       "      <th>nas_delay</th>\n",
       "      <th>security_delay</th>\n",
       "      <th>late_aircraft_delay</th>\n",
       "      <th>total_add_gtime</th>\n",
       "      <th>longest_add_gtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>OO</td>\n",
       "      <td>OKC</td>\n",
       "      <td>SLC</td>\n",
       "      <td>839</td>\n",
       "      <td>830.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1028</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_date airline origin dest  crs_dep_time  dep_time  dep_delay  \\\n",
       "0  2018-12-26      OO    OKC  SLC           839     830.0       -9.0   \n",
       "\n",
       "   taxi_out  wheels_off  wheels_on  taxi_in  crs_arr_time  arr_time  \\\n",
       "0      15.0       845.0      945.0     25.0          1028    1010.0   \n",
       "\n",
       "   arr_delay  crs_elapsed_time  actual_elapsed_time  air_time  distance  \\\n",
       "0      -18.0             169.0                160.0     120.0     866.0   \n",
       "\n",
       "   carrier_delay  weather_delay  nas_delay  security_delay  \\\n",
       "0            0.0            0.0        0.0             0.0   \n",
       "\n",
       "   late_aircraft_delay  total_add_gtime  longest_add_gtime  \n",
       "0                  0.0              0.0                0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3921 outliers removed'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>fl_month</th>\n",
       "      <th>airline_delay</th>\n",
       "      <th>haul_length</th>\n",
       "      <th>dep_timeday</th>\n",
       "      <th>arr_timeday</th>\n",
       "      <th>busy_origin</th>\n",
       "      <th>busy_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.0</td>\n",
       "      <td>866.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arr_delay  distance  fl_month  airline_delay  haul_length  dep_timeday  \\\n",
       "0      -18.0     866.0        12              1            0            1   \n",
       "1       -6.0     284.0         3              1            0            1   \n",
       "2      -13.0     649.0        12              1            0            3   \n",
       "3      -19.0     588.0         3              1            0            2   \n",
       "4        3.0     893.0        10              1            0            3   \n",
       "\n",
       "   arr_timeday  busy_origin  busy_dest  \n",
       "0            1            3          3  \n",
       "1            1            1          3  \n",
       "2            3            3          4  \n",
       "3            2            3          3  \n",
       "4            0            4          2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(241793, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove outliers to 3stdev of arrival delay mean to avoid skewed data in the model\n",
    "rem_outliers(flights, 'arr_delay')\n",
    "\n",
    "# convert date to datetime in order to grab the month\n",
    "flights['fl_date'] = pd.to_datetime(flights['fl_date'])\n",
    "flights['fl_month'] = flights['fl_date'].dt.month\n",
    "flights.drop(columns='fl_date', inplace=True) # this won't be needed after we get month\n",
    "\n",
    "# set delay rating based on expected performance of the airline\n",
    "airline_rating = pd.read_csv('data/airline_delay_rating.csv',index_col=0)\n",
    "flights = flights.merge(airline_rating, left_on='op_unique_carrier', right_on='airline', how='inner')\n",
    "flights.drop(columns=['op_unique_carrier'],inplace=True) \n",
    "# drop airlines since we are calculating delay level based on their mean late carrier info\n",
    "\n",
    "# obtain haul length of the flight\n",
    "haul(flights, 'crs_elapsed_time')\n",
    "# drop columns relating to mins of flight now that we have categories for length\n",
    "flights.drop(columns=['crs_elapsed_time','actual_elapsed_time','air_time'],inplace=True)\n",
    "\n",
    "# drop more columns we will not have in the prediction\n",
    "flights.drop(columns=['taxi_out','taxi_in','arr_time','dep_delay','carrier_delay','weather_delay','nas_delay','security_delay','late_aircraft_delay','total_add_gtime','longest_add_gtime'], inplace=True)\n",
    "\n",
    "# new column of categorical time of day information\n",
    "flights['dep_timeday'] = time_day(flights, 'crs_dep_time')\n",
    "flights['arr_timeday'] = time_day(flights, 'crs_arr_time')\n",
    "# drop info relating to time of day hhmm since we now have time of day categories for dep and arrival\n",
    "flights.drop(columns=['crs_dep_time','dep_time','wheels_off','wheels_on','crs_arr_time'],inplace=True)\n",
    "\n",
    "# classify the business of the origin and departure airports\n",
    "origin_traffic = pd.read_csv('data/origin_traffic_rating.csv', index_col=0)\n",
    "flights = flights.merge(origin_traffic, left_on='origin', right_on='origin', how='left')\n",
    "dest_traffic = pd.read_csv('data/dest_traffic_rating.csv', index_col=0)\n",
    "flights = flights.merge(dest_traffic, left_on='dest', right_on='dest', how='left')\n",
    "\n",
    "flights.drop(columns='airline',inplace=True) \n",
    "flights.drop(columns=['origin','dest'],inplace=True)\n",
    "\n",
    "# have a look at the dataset\n",
    "flights.head()\n",
    "flights.shape\n",
    "flights.to_csv('data/flights250k_cleaned.csv')\n",
    "# new functions\n",
    "#airline_rating = pd.DataFrame.from_dict(airline_delay(flights, 'dep_delay', 'airline'), orient='index', columns=['airline_delay']).reset_index().rename(columns={'index':'airline'})\n",
    "#airline_rating.to_csv('data/airline_delay_rating.csv')\n",
    "#busy_rating(flights, 'origin')\n",
    "#busy_rating(flights, 'dest')\n",
    "# pulled these down from above to allow me to run new functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>fl_month</th>\n",
       "      <th>airline_delay</th>\n",
       "      <th>haul_length</th>\n",
       "      <th>dep_timeday</th>\n",
       "      <th>arr_timeday</th>\n",
       "      <th>busy_origin</th>\n",
       "      <th>busy_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.0</td>\n",
       "      <td>866.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arr_delay  distance  fl_month  airline_delay  haul_length  dep_timeday  \\\n",
       "0      -18.0     866.0        12              1            0            1   \n",
       "1       -6.0     284.0         3              1            0            1   \n",
       "2      -13.0     649.0        12              1            0            3   \n",
       "3      -19.0     588.0         3              1            0            2   \n",
       "4        3.0     893.0        10              1            0            3   \n",
       "\n",
       "   arr_timeday  busy_origin  busy_dest  \n",
       "0            1            3          3  \n",
       "1            1            1          3  \n",
       "2            3            3          4  \n",
       "3            2            3          3  \n",
       "4            0            4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(241793, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = pd.read_csv('data/flights250k_cleaned.csv', index_col=0)\n",
    "flights.head()\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = flights.drop(columns=['arr_delay'])\n",
    "y = flights['arr_delay']\n",
    "rob_sc = RobustScaler()\n",
    "X_sc = rob_sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply different selection techniques to find out which one will be the best for our problems.\n",
    "\n",
    "- Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.55475262e+01, -5.41267690e+00,  1.14449916e+00, ...,\n",
       "         3.13594698e-02,  1.44604544e-01, -2.82883470e-01],\n",
       "       [-4.86452683e+02,  3.58925480e+00,  1.27747777e+00, ...,\n",
       "         5.02443375e-01,  1.31939120e+00,  2.50263536e-02],\n",
       "       [-1.21452407e+02, -5.40819480e+00, -1.58346101e+00, ...,\n",
       "         5.25132727e-01, -6.27264268e-01, -1.49515667e-01],\n",
       "       ...,\n",
       "       [ 2.15471223e+01, -1.40993424e+00, -1.45229307e+00, ...,\n",
       "         7.50332971e-01,  2.59605695e+00, -2.89151796e-01],\n",
       "       [ 2.15472738e+01,  5.87810733e-01,  2.66589399e-01, ...,\n",
       "        -1.23669487e+00,  2.83922993e+00, -2.19231685e-01],\n",
       "       [ 2.15472030e+01,  2.58852297e+00, -3.06561914e-01, ...,\n",
       "        -4.81769096e-01,  2.78908454e+00, -2.20223888e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(random_state = 42)\n",
    "pca.fit_transform(X)\n",
    "eigenvalues = pca.explained_variance_\n",
    "k = 0\n",
    "for val in eigenvalues:\n",
    "    if val > 1:\n",
    "        k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = k, random_state = 42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "Xpca_train, Xpca_test, ypca_train, ypca_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different ML techniques to predict each problem.\n",
    "\n",
    "- linear / logistic / multinomial logistic regression\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- SVM\n",
    "- XGBoost\n",
    "- The ensemble of your own choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Logisitic Regression\": LogisticRegression(),\n",
    "    \"Support Vector Regression\": SVR(),\n",
    "    \"Random Forest Regression\": RandomForestRegressor(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": xgb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Cross Validation Score:  1.86%\n",
      "training R2 / Variance:  0.02\n",
      "Residual Sum of Squares: 786.24\n",
      "\n",
      "Linear Regression - y_test\n",
      "R2 Score \t0.02\n",
      "Test RMSE \t786.24\n",
      "Test MAE \t18.63\n",
      "\n",
      "CPU times: user 1.05 s, sys: 1 s, total: 2.05 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Regression on Original Features\n",
    "lin_params = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid_lin = GridSearchCV(LinearRegression(), lin_params, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_lin.fit(X_train, y_train)\n",
    "linreg = grid_lin.best_estimator_\n",
    "\n",
    "linreg_score = cross_val_score(linreg, X_train, y_train, cv=5)\n",
    "print('Cross Validation Score: ', round(linreg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"training R2 / Variance: \", round(grid_lin.best_score_,2))\n",
    "print(f\"Residual Sum of Squares: {round(np.mean((grid_lin.predict(X_test) - y_test) ** 2),2)}\")\n",
    "\n",
    "y_linreg = linreg.predict(X_test)\n",
    "print('\\nLinear Regression - y_test')\n",
    "print('R2 Score \\t{:.2f}'.format(metrics.r2_score(y_test, y_linreg)))\n",
    "print('Test RMSE \\t{:.2f}'.format(metrics.mean_squared_error(y_test, y_linreg)))\n",
    "print('Test MAE \\t{:.2f}\\n'.format(metrics.mean_absolute_error(y_test, y_linreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Cross Validation Score:  1.07%\n",
      "Training R2 / Variance:  0.01\n",
      "Residual Sum of Squares (RSS): 793.42\n",
      "\n",
      "Linear Regression - ypca_test\n",
      "R2 Score \t0.01\n",
      "RMSE Score \t793.42\n",
      "MAE Score \t18.70\n",
      "\n",
      "CPU times: user 547 ms, sys: 750 ms, total: 1.3 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Regression on PCA Components\n",
    "lin_params = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid_linp = GridSearchCV(LinearRegression(), lin_params, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_linp.fit(Xpca_train, ypca_train)\n",
    "plinreg = grid_linp.best_estimator_\n",
    "\n",
    "plinreg_score = cross_val_score(plinreg, Xpca_train, ypca_train, cv=5)\n",
    "print('Cross Validation Score: ', round(plinreg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_linp.best_score_,2))\n",
    "print(f\"Residual Sum of Squares (RSS): {round(np.mean((grid_linp.predict(Xpca_test) - ypca_test) ** 2),2)}\")\n",
    "\n",
    "ypca_linreg = plinreg.predict(Xpca_test)\n",
    "print('\\nLinear Regression - ypca_test')\n",
    "print('R2 Score \\t{:.2f}'.format(metrics.r2_score(ypca_test, ypca_linreg)))\n",
    "print('RMSE Score \\t{:.2f}'.format(metrics.mean_squared_error(ypca_test, ypca_linreg)))\n",
    "print('MAE Score \\t{:.2f}\\n'.format(metrics.mean_absolute_error(ypca_test, ypca_linreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression \n",
    "log_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid_log = GridSearchCV(LogisticRegression(), log_params, cv=3)\n",
    "grid_log.fit(X_train, y_train)\n",
    "logreg = grid_log.best_estimator_\n",
    "logreg_score = cross_val_score(logreg, X_train, y_train, cv=3, verbose=1, njobs=-1)\n",
    "print('Logistic Regression Cross Validation Score: ', round(logreg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_log.best_score_,2))\n",
    "print(f\"Residual sum of squares: {round(np.mean((grid_log.predict(X_test) - y_test) ** 2),2)}\")\n",
    "\n",
    "y_logreg = logreg.predict(X_test)\n",
    "print('\\nLogistic Regression - y_test')\n",
    "print('Test R2 Score \\t{:.2f}\\n'.format(metrics.r2_score(y_test, y_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression \n",
    "log_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid_logp = GridSearchCV(LogisticRegression(), log_params, cv=5)\n",
    "grid_logp.fit(Xpca_train, ypca_train)\n",
    "plogreg = grid_logp.best_estimator_\n",
    "plogreg_score = cross_val_score(plogreg, Xpca_train, ypca_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: ', round(plogreg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_logp.best_score_,2))\n",
    "print(f\"Residual sum of squares: {round(np.mean((grid_logp.predict(Xpca_test) - ypca_test) ** 2),2)}\")\n",
    "\n",
    "ypca_logreg = plogreg.predict(Xpca_test)\n",
    "print('\\nLogistic Regression - ypca_test')\n",
    "print('Test R2 Score \\t{:.2f}\\n'.format(metrics.r2_score(ypca_test, ypca_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree_params = {\"n_estimators\": [100, 250, 500, 750, 1000],\n",
    "               'max_depth': [int(x) for x in np.linspace(1, 32, num = 5)]}\n",
    "grid_tree = GridSearchCV(RandomForestRegressor(), tree_params, cv=3, verbose=1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "forest = grid_tree.best_estimator_\n",
    "forest_score = cross_val_score(forest, X_train, y_train, cv=3)\n",
    "print('Random Forest Regressor Cross Validation Score: ', round(forest_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_tree.best_score_,2))\n",
    "print(f\"Residual Sum of Squares (RSS): {round(np.mean((grid_tree.predict(X_test) - y_test) ** 2),2)}\")\n",
    "\n",
    "y_forest = forest.predict(X_test)\n",
    "print('\\nRandom Forest Regressor - y_test')\n",
    "print('Test R2 Score \\t{:.2f}'.format(metrics.r2_score(y_test, y_forest)))\n",
    "print('Test RMSE \\t{:.2f}'.format(metrics.mean_squared_error(y_test, y_forest)))\n",
    "print('Test MAE \\t{:.2f}\\n'.format(metrics.mean_absolute_error(y_test, y_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'njobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'njobs'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree_params = {\"n_estimators\": [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 100, num = 10)]}\n",
    "grid_ptree = GridSearchCV(RandomForestRegressor(), tree_params, cv=3, verbose=1, n_jobs=-1)\n",
    "grid_ptree.fit(Xpca_train, ypca_train)\n",
    "pforest = grid_ptree.best_estimator_\n",
    "pforest_score = cross_val_score(pforest, Xpca_train, ypca_train, cv=3)\n",
    "print('Random Forest Regressor Cross Validation Score: ', round(pforest_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_ptree.best_score_,2))\n",
    "print(f\"Residual Sum of Squares (RSS): {round(np.mean((grid_ptree.predict(Xpca_test) - ypca_test) ** 2),2)}\")\n",
    "\n",
    "y_pforest = pforest.predict(Xpca_test)\n",
    "print('\\nRandom Forest Regressor - ypca_test')\n",
    "print('Test R2 Score \\t{:.2f}\\n'.format(metrics.r2_score(ypca_test, y_pforest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'cache_size', 'coef0', 'degree', 'epsilon', 'gamma', 'kernel', 'max_iter', 'shrinking', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVR().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svr_params = {'kernel': ['linear','rbf','poly','sigmoid'],\n",
    "              'C': [1, 5, 10],\n",
    "              'gamma': [0.0000001, 0.00001, 0.001, 0.1, 1],\n",
    "              'epsilon': [0.1, 0.2, 0.3, 0.5, ]\n",
    "             }\n",
    "grid_svr = GridSearchCV(SVR(), svr_params, cv=3, verbose=2, n_jobs=-1)\n",
    "grid_svr.fit(X_train, y_train)\n",
    "svr = grid_svr.best_estimator_\n",
    "svr_score = cross_val_score(svr, X_train, y_train, cv=3)\n",
    "print('Support Vector Regressor (SVR) Cross Validation Score: ', round(svr_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", round(grid_svr.best_score_,2))\n",
    "print(f\"Residual Sum of Squares (RSS): {round(np.mean((grid_svr.predict(X_test) - y_test) ** 2),2)}\")\n",
    "\n",
    "y_svr = svr.predict(X_test)\n",
    "print('\\nSupport Vector Regressor (SVR) - y_test')\n",
    "print('Test R2 Score \\t{:.2f}\\n'.format(metrics.r2_score(y_test, y_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,  y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc));\n",
    "plt.legend(loc=4);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg Results\n",
      "Recall \t\t0.72\n",
      "Precision \t0.90\n",
      "F1 Score \t0.80\n",
      "Accuracy \t0.88\n",
      "R2 Score \t0.46\n"
     ]
    }
   ],
   "source": [
    "print('Log Class Results')\n",
    "print('Recall \\t\\t{:.2f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "print('Precision \\t{:.2f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "print('F1 Score \\t{:.2f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "print('Accuracy \\t{:.2f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('R2 Score \\t{:.2f}'.format(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have data from 2018 and 2019 to develop models. Use different evaluation metrics for each problem and compare the performance of different models.\n",
    "\n",
    "You are required to predict delays on **out of sample** data from **first 7 days (1st-7th) of January 2020** and to share the file with LighthouseLabs. Sample submission can be found in the file **_sample_submission.csv_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660556, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_test = pd.read_csv('data/flighttest.csv', index_col=0)\n",
    "#flight_test.head()\n",
    "flight_test.shape\n",
    "#whatsleft(flight_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning mimicked from above, adjusted where appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>fl_month</th>\n",
       "      <th>airline_delay</th>\n",
       "      <th>haul_length</th>\n",
       "      <th>dep_timeday</th>\n",
       "      <th>arr_timeday</th>\n",
       "      <th>busy_origin</th>\n",
       "      <th>busy_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance  fl_month  airline_delay  haul_length  dep_timeday  arr_timeday  \\\n",
       "0       363         1              2            0            3            3   \n",
       "1       363         1              2            0            2            2   \n",
       "2       333         1              2            0            3            3   \n",
       "3       333         1              2            0            2            2   \n",
       "4       333         1              2            0            1            1   \n",
       "\n",
       "   busy_origin  busy_dest  \n",
       "0          3.0        4.0  \n",
       "1          3.0        4.0  \n",
       "2          3.0        3.0  \n",
       "3          3.0        3.0  \n",
       "4          3.0        3.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(660556, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only NaN-contianing column is tail_num which is to be dropped anyways\n",
    "\n",
    "#columns never used to drop\n",
    "flight_test.drop(columns=['mkt_unique_carrier','tail_num','branded_code_share','mkt_carrier','mkt_carrier_fl_num','op_carrier_fl_num','origin_airport_id','dest_airport_id','dup','flights','origin_city_name','dest_city_name'],inplace=True)\n",
    "\n",
    "# convert date to datetime in order to grab the month\n",
    "flight_test['fl_date'] = pd.to_datetime(flight_test['fl_date'])\n",
    "flight_test['fl_month'] = flight_test['fl_date'].dt.month\n",
    "flight_test.drop(columns='fl_date', inplace=True) # this won't be needed after we get month\n",
    "\n",
    "# set delay rating based on expected performance of the airline\n",
    "flight_test = flight_test.merge(airline_rating, left_on='op_unique_carrier', right_on='airline', how='inner')\n",
    "flight_test.drop(columns=['op_unique_carrier','airline'],inplace=True) \n",
    "\n",
    "# obtain haul length of the flight\n",
    "haul(flight_test, 'crs_elapsed_time')\n",
    "flight_test.drop(columns=['crs_elapsed_time'],inplace=True)\n",
    "\n",
    "# new column of categorical time of day information\n",
    "flight_test['dep_timeday'] = time_day(flight_test, 'crs_dep_time')\n",
    "flight_test['arr_timeday'] = time_day(flight_test, 'crs_arr_time')\n",
    "flight_test.drop(columns=['crs_dep_time','crs_arr_time'],inplace=True)\n",
    "\n",
    "# classify the business of the origin and departure airports\n",
    "flight_test = flight_test.merge(origin_traffic, left_on='origin', right_on='origin', how='left')\n",
    "flight_test = flight_test.merge(dest_traffic, left_on='dest', right_on='dest', how='left')\n",
    "flight_test = flight_test.fillna(flight_test['busy_origin'].mean())\n",
    "#flight_test = flight_test['busy_dest'].fillna(flight_test['busy_dest'].mean())\n",
    "flight_test.drop(columns=['origin','dest'],inplace=True)\n",
    "\n",
    "# have a look at the dataset\n",
    "flight_test.head()\n",
    "flight_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \t\t # Nan Values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsleft(flight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 2.27 s, total: 4.52 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Regression on Original Features\n",
    "lin_params = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid_lin = GridSearchCV(LinearRegression(), lin_params, cv=5)\n",
    "grid_lin.fit(X_sc, y)\n",
    "linreg = grid_lin.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross Validation Score:  1.47%\n",
      "Training R2 / Variance:  0.01548138243857818\n",
      "Residual Sum of Squares: 789.88\n",
      "Linear Regression - y_test\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [241793, 660556]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-980c84e7583f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflight_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear Regression - y_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test R2 Score \\t{:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \"\"\"\n\u001b[1;32m    676\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 677\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    678\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0margument\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 263\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [241793, 660556]"
     ]
    }
   ],
   "source": [
    "linreg_score = cross_val_score(linreg, X_sc, y, cv=5)\n",
    "print('Linear Regression Cross Validation Score: ', round(linreg_score.mean() * 100, 2).astype(str) + '%')\n",
    "print(\"Training R2 / Variance: \", grid_lin.best_score_)\n",
    "print(f\"Residual Sum of Squares: {round(np.mean((grid_lin.predict(X_sc) - y) ** 2),2)}\")\n",
    "y_pred = linreg.predict(flight_test)\n",
    "print('Linear Regression - y_test')\n",
    "print('Test R2 Score \\t{:.2f}'.format(metrics.r2_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "## Stretch Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variables are **CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY**. We need to do additional transformations because these variables are not binary but continuos. For each flight that was delayed, we need to have one of these variables as 1 and others 0.\n",
    "\n",
    "It can happen that we have two types of delays with more than 0 minutes. In this case, take the bigger one as 1 and others as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is **CANCELLED**. The main problem here is going to be huge class imbalance. We have only very little cancelled flights with comparison to all flights. It is important to do the right sampling before training and to choose correct evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
